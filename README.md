# âš¡ Volt-Infer

**Production-Grade Distributed Mixture-of-Experts (MoE) Inference Engine**

Volt-Infer enables large sparse MoE models (like Mixtral 8x7B) to run efficiently across multiple consumer-grade GPUs by distributing expert computation over TCP with custom kernels, speculative prefetching, and intelligent autoscaling.

---

## ğŸ¯ Key Features

### **1. Custom Triton Kernels**
- **Top-K Gating**: GPU-accelerated expert selection, 2-3x faster than PyTorch baseline
- **INT8 Quantization**: On-the-fly weight dequantization for 2x memory savings
- **Zero-Copy Operations**: Minimal serialization overhead

### **2. Speculative Prefetching**
- **N-Gram Predictor**: Learns token-to-expert patterns during inference
- **Latency Masking**: Prefetch expert computation before tokens arrive
- **Adaptive Learning**: Improves accuracy over time

### **3. Paged Attention Routing**
- **KV-Cache Management**: Efficient memory pooling with page tables
- **Copy-on-Write**: Optimal for beam search and multi-sequence batching
- **LRU Eviction**: Automatic memory pressure handling

### **4. Production MLOps**
- **Prometheus Metrics**: P50/P90/P99 latency, queue depth, expert utilization
- **Autoscaling**: Automatic worker provisioning based on queue depth
- **Graceful Degradation**: Continues operation even with failed experts

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Router    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Worker 1   â”‚
â”‚             â”‚         â”‚    Node     â”‚         â”‚ (Expert 0,1)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚             â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚  â€¢ Gating   â”‚         
                        â”‚  â€¢ Routing  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  â€¢ Failover â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Worker 2   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ (Expert 2,3)â”‚
                            â”‚   â–²               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚   â”‚               
                            â–¼   â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Worker N   â”‚
                        â”‚    Redis    â”‚         â”‚ (Expert N-1)â”‚
                        â”‚  Discovery  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Component Roles**

| Component | Responsibility |
|-----------|---------------|
| **Router Node** | Runs gating kernel, dispatches tokens to experts, aggregates results |
| **Worker Node** | Hosts expert weights, processes forward passes on GPU |
| **Redis** | Peer discovery, global state, heartbeat tracking |
| **Prometheus** | Metrics collection for observability and autoscaling |

---

## ğŸ“¦ Installation

### **Prerequisites**
- Python 3.11+
- CUDA 11.8+ (for GPU support)
- Redis 7.0+
- 8GB+ GPU VRAM per worker

### **Install from Source**
```bash
git clone https://github.com/yourusername/volt-infer.git
cd volt-infer

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Verify installation
python -c "import volt; print('âœ“ Volt-Infer installed successfully')"
```

---

## ğŸš€ Quick Start

### **Step 1: Start Redis**
```bash
docker run -d -p 6379:6379 redis:7-alpine
```

### **Step 2: Launch Worker Nodes**
```bash
# Worker 1: Hosts Expert 0 and 1
VOLT_WORKER_EXPERT_IDS="0,1" \
VOLT_NETWORK_PORT=50100 \
python -m volt.runtime.worker

# Worker 2: Hosts Expert 2 and 3
VOLT_WORKER_EXPERT_IDS="2,3" \
VOLT_NETWORK_PORT=50101 \
python -m volt.runtime.worker
```

### **Step 3: Launch Router Node**
```bash
VOLT_MODEL_NUM_EXPERTS=8 \
VOLT_NETWORK_PORT=50051 \
python -m volt.runtime.router
```

### **Step 4: Run Inference**
```python
import torch
from volt.runtime.router import RouterNode
from volt.core.config import VoltConfig, NodeType

# Initialize router
config = VoltConfig.from_env(NodeType.ROUTER, "router-1")
router = RouterNode(config)

await router.start()

# Run inference
hidden_states = torch.randn(32, 4096, device='cuda')  # [batch, hidden_dim]
output = await router.forward(hidden_states)

print(f"Output shape: {output.shape}")  # [32, 4096]
```

---

## âš™ï¸ Configuration

### **Environment Variables**

| Variable | Description | Default |
|----------|-------------|---------|
| `VOLT_NETWORK_PORT` | TCP port for data plane | 50051 |
| `VOLT_REDIS_URL` | Redis connection string | redis://localhost:6379/0 |
| `VOLT_MODEL_NUM_EXPERTS` | Total number of experts | 8 |
| `VOLT_MODEL_TOP_K` | Experts per token | 2 |
| `VOLT_WORKER_EXPERT_IDS` | Comma-separated expert IDs | 0 |
| `VOLT_LOG_LEVEL` | Logging verbosity | INFO |

### **Python Configuration**
```python
from volt.core.config import VoltConfig, NodeType, WorkerConfig

config = VoltConfig(
    node_type=NodeType.WORKER,
    node_id="worker-1",
    worker=WorkerConfig(
        expert_ids=[0, 1],
        device="cuda:0",
        max_batch_size=32,
        queue_depth=64,
    ),
)
```

---

## ğŸ“Š Monitoring

### **Prometheus Metrics**
Access metrics at `http://localhost:9090/metrics`:

```prometheus
# Request latency distribution
voltinfer_request_latency_seconds_bucket{le="0.05"} 245

# Queue depth (autoscaling trigger)
voltinfer_queue_depth{node="worker-1"} 12

# Expert utilization
voltinfer_expert_requests_total{expert_id="3"} 1523
```

### **Grafana Dashboard**
Import the provided dashboard: `monitoring/grafana-dashboard.json`

---

## ğŸ§ª Benchmarking

### **Throughput Test**
```bash
python benchmarks/throughput.py \
    --batch-size 32 \
    --num-iterations 100 \
    --num-experts 8
```

**Expected Results (RTX 4090):**
- **Baseline (PyTorch)**: ~180 tokens/sec
- **Volt-Infer**: ~320 tokens/sec
- **Speedup**: 1.8x

### **Kernel Microbenchmarks**
```bash
python -m volt.kernels.gating
```

---

## ğŸ§© Advanced Features

### **1. Quantization**
Enable INT8 quantization to reduce memory by 50%:
```python
config.model.use_quantization = True
```

### **2. Speculative Prefetching**
```python
from volt.runtime.scheduler import create_scheduler

scheduler = create_scheduler(
    num_experts=8,
    ngram_order=2,
    lookahead_tokens=1,
)
```

### **3. Autoscaling**
```python
from volt.mlops.autoscaler import Autoscaler, AutoscalerConfig

autoscaler = Autoscaler(
    config=AutoscalerConfig(
        queue_depth_threshold=16,
        webhook_url="http://localhost:8080/scale",
    )
)

await autoscaler.start()
```

---

## ğŸ› ï¸ Development

### **Type Checking**
```bash
mypy volt/ --strict
```

### **Code Formatting**
```bash
black volt/
ruff check volt/
```

### **Testing**
```bash
pytest tests/ -v --cov=volt
```

---

## ğŸ“š API Reference

### **Router Node**
```python
class RouterNode:
    async def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """Route tokens through experts."""
        ...
```

### **Worker Node**
```python
class WorkerNode:
    async def start(self) -> None:
        """Start worker server."""
        ...
```

### **Triton Kernels**
```python
def topk_gating_kernel(
    hidden_states: torch.Tensor,
    router_weights: torch.Tensor,
    k: int = 2,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """Compute Top-K expert routing."""
    ...
```

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **vLLM**: Paged attention design inspiration
- **Mixtral**: MoE architecture reference
- **Triton**: GPU kernel framework

---

## ğŸ“ Support

- **Issues**: https://github.com/yourusername/volt-infer/issues
- **Discussions**: https://github.com/yourusername/volt-infer/discussions
- **Email**: support@volt-infer.dev

---

## ğŸ—ºï¸ Roadmap

- [ ] **Q1 2026**: Support for DeepSeek-MoE models
- [ ] **Q2 2026**: Multi-node distributed training
- [ ] **Q3 2026**: WebGPU backend for edge deployment
- [ ] **Q4 2026**: AutoML for router weight optimization

---

**Built with âš¡ by the Volt-Infer Team**
